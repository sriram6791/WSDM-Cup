{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8cd505",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-05T07:35:10.241976Z",
     "iopub.status.busy": "2024-08-05T07:35:10.240978Z",
     "iopub.status.idle": "2024-08-05T07:35:10.247863Z",
     "shell.execute_reply": "2024-08-05T07:35:10.245857Z",
     "shell.execute_reply.started": "2024-08-05T07:35:10.241927Z"
    },
    "papermill": {
     "duration": 0.004946,
     "end_time": "2025-01-30T09:41:55.615276",
     "exception": false,
     "start_time": "2025-01-30T09:41:55.610330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e36d969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:41:55.625366Z",
     "iopub.status.busy": "2025-01-30T09:41:55.624781Z",
     "iopub.status.idle": "2025-01-30T09:42:05.035111Z",
     "shell.execute_reply": "2025-01-30T09:42:05.034296Z"
    },
    "papermill": {
     "duration": 9.417301,
     "end_time": "2025-01-30T09:42:05.037013",
     "exception": false,
     "start_time": "2025-01-30T09:41:55.619712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip==23.3.2 in /opt/conda/lib/python3.10/site-packages (23.3.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pip==23.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22bd4a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:05.047768Z",
     "iopub.status.busy": "2025-01-30T09:42:05.047491Z",
     "iopub.status.idle": "2025-01-30T09:42:17.419343Z",
     "shell.execute_reply": "2025-01-30T09:42:17.418518Z"
    },
    "papermill": {
     "duration": 12.379102,
     "end_time": "2025-01-30T09:42:17.420979",
     "exception": false,
     "start_time": "2025-01-30T09:42:05.041877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton==2.2.0) (3.13.1)\r\n",
      "Installing collected packages: triton\r\n",
      "Successfully installed triton-2.2.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38620933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:17.432270Z",
     "iopub.status.busy": "2025-01-30T09:42:17.431579Z",
     "iopub.status.idle": "2025-01-30T09:42:32.903838Z",
     "shell.execute_reply": "2025-01-30T09:42:32.903066Z"
    },
    "papermill": {
     "duration": 15.47952,
     "end_time": "2025-01-30T09:42:32.905501",
     "exception": false,
     "start_time": "2025-01-30T09:42:17.425981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch>=1.12 in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.24042abc8.d20240802) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.12->xformers==0.0.24042abc8.d20240802) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12->xformers==0.0.24042abc8.d20240802) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12->xformers==0.0.24042abc8.d20240802) (1.3.0)\r\n",
      "Installing collected packages: xformers\r\n",
      "Successfully installed xformers-0.0.24+042abc8.d20240802\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install /kaggle/input/lmsys-packages/xformers-0.0.24042abc8.d20240802-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaccdf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:32.917335Z",
     "iopub.status.busy": "2025-01-30T09:42:32.916768Z",
     "iopub.status.idle": "2025-01-30T09:42:33.502117Z",
     "shell.execute_reply": "2025-01-30T09:42:33.501213Z"
    },
    "papermill": {
     "duration": 0.593291,
     "end_time": "2025-01-30T09:42:33.504116",
     "exception": false,
     "start_time": "2025-01-30T09:42:32.910825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/lmsys-modules-0805 human_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9e35fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:33.515791Z",
     "iopub.status.busy": "2025-01-30T09:42:33.515515Z",
     "iopub.status.idle": "2025-01-30T09:42:42.416208Z",
     "shell.execute_reply": "2025-01-30T09:42:42.415400Z"
    },
    "papermill": {
     "duration": 8.908766,
     "end_time": "2025-01-30T09:42:42.418204",
     "exception": false,
     "start_time": "2025-01-30T09:42:33.509438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.11.1\r\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (2.1.2)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (4.42.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (4.66.4)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.32.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.4.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.23.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2024.5.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.11.1) (3.1.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.1) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.1) (0.19.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2024.7.4)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\r\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: peft\r\n",
      "Successfully installed peft-0.11.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f858c406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:42.431021Z",
     "iopub.status.busy": "2025-01-30T09:42:42.430738Z",
     "iopub.status.idle": "2025-01-30T09:42:42.437695Z",
     "shell.execute_reply": "2025-01-30T09:42:42.437096Z"
    },
    "papermill": {
     "duration": 0.015116,
     "end_time": "2025-01-30T09:42:42.439158",
     "exception": false,
     "start_time": "2025-01-30T09:42:42.424042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting human_pref/data/processors.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile human_pref/data/processors.py\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class ProcessorPAB:\n",
    "    PROMPT_PREFIX = \"\"\"Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user’s instructions and answers the user’s question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\"\"\"\n",
    "\n",
    "    PROMPT_SUFFIX = \"verdict is: [[\"\n",
    "\n",
    "    LABEL_COLS = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "    def __init__(self, tokenizer, max_length, support_system_role):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.support_system_role = support_system_role\n",
    "\n",
    "    def build_conversation(self, prompts, responses_a, responses_b):\n",
    "        head = \"<|The Start of Conversation between a User and two Assistants|>\"\n",
    "        tail = \"<|The End of Conversation between a User and two Assistants|>\\n\"\n",
    "        parts = []\n",
    "        for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):\n",
    "            if prompt is None:\n",
    "                prompt = \"null\"\n",
    "            if response_a is None:\n",
    "                response_a = \"null\"\n",
    "            if response_b is None:\n",
    "                response_b = \"null\"\n",
    "            parts.append(\n",
    "                f\"\\n### User:\\n{prompt}\\n\\n### Assistant A:\\n{response_a}\\n\\n### Assistant B:\\n{response_b}\\n\"\n",
    "            )\n",
    "        text = \"\".join(parts)\n",
    "        input_ids = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        ).input_ids\n",
    "\n",
    "        truncated_text = self.tokenizer.decode(input_ids)\n",
    "        return head + truncated_text + tail\n",
    "\n",
    "    def build_input(self, data):\n",
    "        conversation = self.build_conversation(\n",
    "            [data[\"prompt\"]],\n",
    "            [data[\"response_a\"]],\n",
    "            [data[\"response_b\"]],\n",
    "        )\n",
    "        if self.support_system_role:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.PROMPT_PREFIX},\n",
    "                {\"role\": \"user\", \"content\": conversation},\n",
    "            ]\n",
    "        else:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": f\"{self.PROMPT_PREFIX}\\n{conversation}\"},\n",
    "            ]\n",
    "        input_text = (\n",
    "            self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            + self.PROMPT_SUFFIX\n",
    "        )\n",
    "        input_ids = self.tokenizer(\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "        label = torch.tensor([data[col] for col in self.LABEL_COLS]).float()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            input_text=input_text,\n",
    "            label=label,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d6eb9",
   "metadata": {
    "papermill": {
     "duration": 0.006287,
     "end_time": "2025-01-30T09:42:42.450974",
     "exception": false,
     "start_time": "2025-01-30T09:42:42.444687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f4f070d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:42.462970Z",
     "iopub.status.busy": "2025-01-30T09:42:42.462711Z",
     "iopub.status.idle": "2025-01-30T09:42:42.467121Z",
     "shell.execute_reply": "2025-01-30T09:42:42.466515Z"
    },
    "papermill": {
     "duration": 0.012143,
     "end_time": "2025-01-30T09:42:42.468599",
     "exception": false,
     "start_time": "2025-01-30T09:42:42.456456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prepare_test_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_test_file.py\n",
    "import pandas as pd\n",
    "import os\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "print(\"IS_SUBMISSION:\", IS_SUBMISSION)\n",
    "\n",
    "if IS_SUBMISSION:\n",
    "    df = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\")\n",
    "else:\n",
    "    df = pd.read_parquet(\"/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet\")\n",
    "    df = df.sample(n=100, random_state=42)\n",
    "\n",
    "df[\"winner_model_a\"] = 1\n",
    "df[\"winner_model_b\"] = 0\n",
    "df[\"winner_tie\"] = 0\n",
    "df.to_parquet(\"test.parquet\", index=False)\n",
    "df[\"response_a\"], df[\"response_b\"] = df[\"response_b\"], df[\"response_a\"]\n",
    "df.to_parquet(\"test_swap.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22fd3644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:42.480478Z",
     "iopub.status.busy": "2025-01-30T09:42:42.480267Z",
     "iopub.status.idle": "2025-01-30T09:42:45.974341Z",
     "shell.execute_reply": "2025-01-30T09:42:45.973481Z"
    },
    "papermill": {
     "duration": 3.50236,
     "end_time": "2025-01-30T09:42:45.976509",
     "exception": false,
     "start_time": "2025-01-30T09:42:42.474149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS_SUBMISSION: False\r\n"
     ]
    }
   ],
   "source": [
    "!python prepare_test_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9729053",
   "metadata": {
    "papermill": {
     "duration": 0.005566,
     "end_time": "2025-01-30T09:42:45.988306",
     "exception": false,
     "start_time": "2025-01-30T09:42:45.982740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: gemma2-9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af2b17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:46.000979Z",
     "iopub.status.busy": "2025-01-30T09:42:46.000665Z",
     "iopub.status.idle": "2025-01-30T09:42:46.007729Z",
     "shell.execute_reply": "2025-01-30T09:42:46.006978Z"
    },
    "papermill": {
     "duration": 0.015513,
     "end_time": "2025-01-30T09:42:46.009397",
     "exception": false,
     "start_time": "2025-01-30T09:42:45.993884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_m0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_m0.py\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "from human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "# Configuration\n",
    "base_model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b-it/2\"  # Base model path\n",
    "lora_path = \"/kaggle/input/o_gemma-2-9b-it-4bit_trained_with_bigdata_v2/transformers/default/4\"  # LoRA adapter path\n",
    "csv_path = \"test.parquet\"\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "processor = ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=False,\n",
    ")\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192, base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Device mapping configuration\n",
    "num_hidden_layers = 42\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "# Load base model with device placement\n",
    "base_model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# # Verify device placement\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"lora\" in name.lower():\n",
    "#         layer_num = int(name.split(\".\")[3])  # Extract layer number from param name\n",
    "#         device = \"cuda:0\" if layer_num < num_hidden_layers//2 else \"cuda:1\"\n",
    "#         assert param.device == torch.device(device), f\"Parameter {name} on wrong device\"\n",
    "\n",
    "# Prepare frequency buffers\n",
    "config = model.config\n",
    "dim = config.head_dim\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "# Pipeline inference\n",
    "t1 = time.time()\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "        \n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "# Process final batch\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "# Post-process results\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "np.save('prob_m0.npy', prob)\n",
    "\n",
    "# Timing results\n",
    "t2 = time.time()\n",
    "print(f\"elapsed time: {t2 - t1}s\")\n",
    "print(f\"elapsed time 10k: {(t2 - t1) * 100 / 3600}h\")\n",
    "print(f\"elapsed time 25k: {(t2 - t1) * 250 / 3600}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a73c09d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:42:46.021770Z",
     "iopub.status.busy": "2025-01-30T09:42:46.021541Z",
     "iopub.status.idle": "2025-01-30T09:55:40.841518Z",
     "shell.execute_reply": "2025-01-30T09:55:40.840666Z"
    },
    "papermill": {
     "duration": 774.828381,
     "end_time": "2025-01-30T09:55:40.843569",
     "exception": false,
     "start_time": "2025-01-30T09:42:46.015188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|                          | 0/4 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:215: UserWarning: \r\n",
      "NVIDIA L4 with CUDA capability sm_89 is not compatible with the current PyTorch installation.\r\n",
      "The current PyTorch install supports CUDA capabilities sm_60 sm_70 sm_75 compute_70 compute_75.\r\n",
      "If you want to use the NVIDIA L4 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\r\n",
      "\r\n",
      "  warnings.warn(\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:18<00:00,  4.56s/it]\r\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma-2/transformers/gemma-2-9b-it/2 and are newly initialized: ['model.layers.0.mlp.gate_up_proj.weight', 'model.layers.1.mlp.gate_up_proj.weight', 'model.layers.10.mlp.gate_up_proj.weight', 'model.layers.11.mlp.gate_up_proj.weight', 'model.layers.12.mlp.gate_up_proj.weight', 'model.layers.13.mlp.gate_up_proj.weight', 'model.layers.14.mlp.gate_up_proj.weight', 'model.layers.15.mlp.gate_up_proj.weight', 'model.layers.16.mlp.gate_up_proj.weight', 'model.layers.17.mlp.gate_up_proj.weight', 'model.layers.18.mlp.gate_up_proj.weight', 'model.layers.19.mlp.gate_up_proj.weight', 'model.layers.2.mlp.gate_up_proj.weight', 'model.layers.20.mlp.gate_up_proj.weight', 'model.layers.21.mlp.gate_up_proj.weight', 'model.layers.22.mlp.gate_up_proj.weight', 'model.layers.23.mlp.gate_up_proj.weight', 'model.layers.24.mlp.gate_up_proj.weight', 'model.layers.25.mlp.gate_up_proj.weight', 'model.layers.26.mlp.gate_up_proj.weight', 'model.layers.27.mlp.gate_up_proj.weight', 'model.layers.28.mlp.gate_up_proj.weight', 'model.layers.29.mlp.gate_up_proj.weight', 'model.layers.3.mlp.gate_up_proj.weight', 'model.layers.30.mlp.gate_up_proj.weight', 'model.layers.31.mlp.gate_up_proj.weight', 'model.layers.32.mlp.gate_up_proj.weight', 'model.layers.33.mlp.gate_up_proj.weight', 'model.layers.34.mlp.gate_up_proj.weight', 'model.layers.35.mlp.gate_up_proj.weight', 'model.layers.36.mlp.gate_up_proj.weight', 'model.layers.37.mlp.gate_up_proj.weight', 'model.layers.38.mlp.gate_up_proj.weight', 'model.layers.39.mlp.gate_up_proj.weight', 'model.layers.4.mlp.gate_up_proj.weight', 'model.layers.40.mlp.gate_up_proj.weight', 'model.layers.41.mlp.gate_up_proj.weight', 'model.layers.5.mlp.gate_up_proj.weight', 'model.layers.6.mlp.gate_up_proj.weight', 'model.layers.7.mlp.gate_up_proj.weight', 'model.layers.8.mlp.gate_up_proj.weight', 'model.layers.9.mlp.gate_up_proj.weight', 'score.1.bias', 'score.1.weight', 'score.4.bias', 'score.4.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]2025-01-30 09:53:03.569361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-01-30 09:53:03.569370: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-01-30 09:53:03.569636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-01-30 09:53:03.569648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-01-30 09:53:03.687278: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "2025-01-30 09:53:03.687273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "100%|█████████████████████████████████████████████| 2/2 [02:36<00:00, 78.28s/it]\r\n",
      "{'log_loss': 4.104305187592326}\r\n",
      "elapsed time: 157.31683492660522s\r\n",
      "elapsed time 10k: 4.36991208129459h\r\n",
      "elapsed time 25k: 10.924780203236473h\r\n"
     ]
    }
   ],
   "source": [
    "!python predict_m0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8eb5a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:55:40.858582Z",
     "iopub.status.busy": "2025-01-30T09:55:40.858316Z",
     "iopub.status.idle": "2025-01-30T09:55:40.864010Z",
     "shell.execute_reply": "2025-01-30T09:55:40.863414Z"
    },
    "papermill": {
     "duration": 0.01501,
     "end_time": "2025-01-30T09:55:40.865405",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.850395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%writefile predict_m0.py\n",
    "# import time\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "# from human_pref.models.modeling_gemma2 import Gemma2ForSequenceClassification\n",
    "# from human_pref.data.processors import ProcessorPAB\n",
    "# from human_pref.data.dataset import LMSYSDataset\n",
    "# from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "# from human_pref.utils import to_device\n",
    "\n",
    "\n",
    "# model_name_or_path = \"/kaggle/input/o_gemma-2-9b-it-4bit_trained_with_bigdata_v2/transformers/default/2\"\n",
    "# csv_path = \"test.parquet\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "# processor = ProcessorPAB(\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=4096,\n",
    "#     support_system_role=False,\n",
    "# )\n",
    "# dataset = LMSYSDataset(\n",
    "#     csv_file=csv_path,\n",
    "#     query=None,\n",
    "#     processor=processor,\n",
    "#     include_swap=False,\n",
    "#     is_parquet=True,\n",
    "# )\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=80,\n",
    "#     num_workers=4,\n",
    "#     collate_fn=ShardedMaxTokensCollator(\n",
    "#         max_tokens=8192, base_collator=VarlenCollator()\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# # model for pipelined inference\n",
    "# num_hidden_layers = 42\n",
    "# device_map = {\n",
    "#     \"model.embed_tokens\": \"cuda:0\",\n",
    "#     \"model.norm\": \"cuda:1\",\n",
    "#     \"score\": \"cuda:1\",\n",
    "# }\n",
    "# for i in range(num_hidden_layers // 2):\n",
    "#     device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "# for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "#     device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "# model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=device_map,\n",
    "# )\n",
    "\n",
    "# # inv_freq clones for each device\n",
    "# config = model.config\n",
    "# dim = config.head_dim\n",
    "# inv_freq = 1.0 / (\n",
    "#     config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    "# )\n",
    "# inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "# inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "# # for name, p in model.named_parameters():\n",
    "# #     print(name, p.device)\n",
    "# # for name, b in model.model.named_buffers():\n",
    "# #     print(name, b.device)\n",
    "\n",
    "# # pipeline parallelism with two GPUs\n",
    "# t1 = time.time()\n",
    "# is_first = True\n",
    "# hidden_states = None\n",
    "# outs = []\n",
    "# for batch in tqdm(dataloader):\n",
    "#     for micro_batch in batch:\n",
    "#         input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "#         seq_info = dict(\n",
    "#             cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "#             position_ids=micro_batch[\"position_ids\"],\n",
    "#             max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "#             # attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "#         )\n",
    "#         seq_info = to_device(seq_info, \"cuda:0\")\n",
    "#         if is_first:\n",
    "#             with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#                 prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "#             is_first = False\n",
    "#             prev_seq_info, prev_hidden_states = to_device(\n",
    "#                 [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "#             )\n",
    "#             continue\n",
    "#         with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#             logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "#             hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "#             prev_seq_info, prev_hidden_states = to_device(\n",
    "#                 [seq_info, hidden_states], \"cuda:1\"\n",
    "#             )\n",
    "#             outs.append(logits.cpu())\n",
    "\n",
    "# # last micro-batch\n",
    "# with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#     logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "#     outs.append(logits.cpu())\n",
    "\n",
    "# pred = torch.cat(outs, dim=0)\n",
    "# prob = pred.softmax(-1)\n",
    "# print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "# np.save('prob_m0.npy', prob)\n",
    "\n",
    "# t2 = time.time()\n",
    "# print(f\"elapsed time: {t2 - t1}s\")\n",
    "# print(f\"elapsed time 10k: {(t2 - t1) * 100 / 3600}h\")\n",
    "# print(f\"elapsed time 25k: {(t2 - t1) * 250 / 3600}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0d59b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:55:40.879693Z",
     "iopub.status.busy": "2025-01-30T09:55:40.879203Z",
     "iopub.status.idle": "2025-01-30T09:55:40.882089Z",
     "shell.execute_reply": "2025-01-30T09:55:40.881530Z"
    },
    "papermill": {
     "duration": 0.011709,
     "end_time": "2025-01-30T09:55:40.883438",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.871729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python predict_m0.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c8603",
   "metadata": {
    "papermill": {
     "duration": 0.006197,
     "end_time": "2025-01-30T09:55:40.895928",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.889731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference: llama3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01082d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:55:40.909654Z",
     "iopub.status.busy": "2025-01-30T09:55:40.909416Z",
     "iopub.status.idle": "2025-01-30T09:55:40.915130Z",
     "shell.execute_reply": "2025-01-30T09:55:40.914561Z"
    },
    "papermill": {
     "duration": 0.014332,
     "end_time": "2025-01-30T09:55:40.916549",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.902217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predict_m3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_m3.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from xformers.ops.fmha.attn_bias import BlockDiagonalCausalMask\n",
    "from human_pref.models.modeling_llama import LlamaForSequenceClassification\n",
    "from human_pref.data.processors import ProcessorPAB\n",
    "from human_pref.data.dataset import LMSYSDataset\n",
    "from human_pref.data.collators import VarlenCollator, ShardedMaxTokensCollator\n",
    "from human_pref.utils import to_device\n",
    "\n",
    "\n",
    "model_name_or_path = \"/kaggle/input/lmsys-checkpoints-3-0805\"\n",
    "csv_path = \"test_swap.parquet\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.deprecation_warnings[\n",
    "    \"sequence-length-is-longer-than-the-specified-maximum\"\n",
    "] = True\n",
    "processor = ProcessorPAB(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=4096,\n",
    "    support_system_role=True,\n",
    ")\n",
    "dataset = LMSYSDataset(\n",
    "    csv_file=csv_path,\n",
    "    query=None,\n",
    "    processor=processor,\n",
    "    include_swap=False,\n",
    "    is_parquet=True,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=80,\n",
    "    num_workers=4,\n",
    "    collate_fn=ShardedMaxTokensCollator(\n",
    "        max_tokens=8192, base_collator=VarlenCollator()\n",
    "    ),\n",
    ")\n",
    "\n",
    "# model for pipelined inference\n",
    "num_hidden_layers = 32\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": \"cuda:0\",\n",
    "    \"model.norm\": \"cuda:1\",\n",
    "    \"score\": \"cuda:1\",\n",
    "}\n",
    "for i in range(num_hidden_layers // 2):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:0\"\n",
    "for i in range(num_hidden_layers // 2, num_hidden_layers):\n",
    "    device_map[f\"model.layers.{i}\"] = \"cuda:1\"\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# inv_freq clones for each device\n",
    "config = model.config\n",
    "dim = config.hidden_size // config.num_attention_heads\n",
    "inv_freq = 1.0 / (\n",
    "    config.rope_theta ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim)\n",
    ")\n",
    "inv_freq0 = inv_freq.to(\"cuda:0\")\n",
    "inv_freq1 = inv_freq.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "# for name, p in model.named_parameters():\n",
    "#     print(name, p.device)\n",
    "# for name, b in model.model.named_buffers():\n",
    "#     print(name, b.device)\n",
    "\n",
    "# pipeline parallelism with two GPUs\n",
    "is_first = True\n",
    "hidden_states = None\n",
    "outs = []\n",
    "for batch in tqdm(dataloader):\n",
    "    for micro_batch in batch:\n",
    "        input_ids = to_device(micro_batch[\"input_ids\"], \"cuda:0\")\n",
    "        seq_info = dict(\n",
    "            cu_seqlens=micro_batch[\"cu_seqlens\"],\n",
    "            position_ids=micro_batch[\"position_ids\"],\n",
    "            max_seq_len=micro_batch[\"max_seq_len\"],\n",
    "            attn_bias=BlockDiagonalCausalMask.from_seqlens(micro_batch[\"seq_lens\"]),\n",
    "        )\n",
    "        seq_info = to_device(seq_info, \"cuda:0\")\n",
    "        if is_first:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "                prev_hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "            is_first = False\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, prev_hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            continue\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "            hidden_states = model.forward_part1(input_ids, seq_info, inv_freq0)\n",
    "\n",
    "            prev_seq_info, prev_hidden_states = to_device(\n",
    "                [seq_info, hidden_states], \"cuda:1\"\n",
    "            )\n",
    "            outs.append(logits.cpu())\n",
    "\n",
    "# last micro-batch\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    logits = model.forward_part2(prev_hidden_states, prev_seq_info, inv_freq1)\n",
    "    outs.append(logits.cpu())\n",
    "\n",
    "\n",
    "pred = torch.cat(outs, dim=0)\n",
    "prob = pred.softmax(-1)\n",
    "print(dataset.evaluate(prob.numpy()))\n",
    "\n",
    "np.save('prob_m3.npy', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81f48045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:55:40.930271Z",
     "iopub.status.busy": "2025-01-30T09:55:40.929786Z",
     "iopub.status.idle": "2025-01-30T09:55:40.932486Z",
     "shell.execute_reply": "2025-01-30T09:55:40.931942Z"
    },
    "papermill": {
     "duration": 0.010952,
     "end_time": "2025-01-30T09:55:40.933866",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.922914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python predict_m3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95c3f7",
   "metadata": {
    "papermill": {
     "duration": 0.006411,
     "end_time": "2025-01-30T09:55:40.946885",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.940474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7979227d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:55:40.960826Z",
     "iopub.status.busy": "2025-01-30T09:55:40.960353Z",
     "iopub.status.idle": "2025-01-30T09:55:40.964455Z",
     "shell.execute_reply": "2025-01-30T09:55:40.963854Z"
    },
    "papermill": {
     "duration": 0.012689,
     "end_time": "2025-01-30T09:55:40.965976",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.953287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing make_submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile make_submission.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "df = pd.read_parquet(\"test.parquet\")\n",
    "# preds = np.average(\n",
    "#     [\n",
    "#         np.load(\"prob_m0.npy\"),\n",
    "#         np.load(\"prob_m3.npy\")[:, [1, 0, 2]],\n",
    "#     ],\n",
    "#     axis=0,\n",
    "#     weights=[2, 1],\n",
    "# )\n",
    "\n",
    "preds = np.load(\"prob_m0.npy\")\n",
    "winters = []\n",
    "for i in range(len(preds)):\n",
    "    winters.append(\"model_a\" if  preds[i, 0] > preds[i, 1] else \"model_b\")\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": df[\"id\"],\n",
    "    \"winner\": winters\n",
    "})\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "if not IS_SUBMISSION:\n",
    "    correct_preds = (df['winner'] == sub['winner']).sum()\n",
    "    total_preds = len(df)\n",
    "    acc = correct_preds / total_preds\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4779358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T09:55:40.979878Z",
     "iopub.status.busy": "2025-01-30T09:55:40.979401Z",
     "iopub.status.idle": "2025-01-30T09:55:42.169288Z",
     "shell.execute_reply": "2025-01-30T09:55:42.168447Z"
    },
    "papermill": {
     "duration": 1.198872,
     "end_time": "2025-01-30T09:55:42.171188",
     "exception": false,
     "start_time": "2025-01-30T09:55:40.972316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\r\n",
      "                                                  id   winner\r\n",
      "0  98aa6a2c252836e0f5fb4412f56e711b9a4d77df15be08...  model_b\r\n",
      "1  c2ba18250bee661dfbe70ab08f6160c79d9e2ac2448a16...  model_b\r\n",
      "2  e8d9b9e71fe263f4b3d6639faf701c259061366033955f...  model_b\r\n",
      "3  06ba8de0f99115628320a0d57ab954ffb3d3af2d010f09...  model_b\r\n",
      "4  9dc583fcf54d9365c05b6f7e59710dfee973b01a283f3a...  model_b\r\n"
     ]
    }
   ],
   "source": [
    "!python make_submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe060d8",
   "metadata": {
    "papermill": {
     "duration": 0.006381,
     "end_time": "2025-01-30T09:55:42.184466",
     "exception": false,
     "start_time": "2025-01-30T09:55:42.178085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 5493674,
     "sourceId": 9102725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496762,
     "sourceId": 9107824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496847,
     "sourceId": 9107963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5496920,
     "sourceId": 9108069,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72256,
     "sourceId": 104453,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 231147,
     "modelInstanceId": 209460,
     "sourceId": 245501,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 829.218022,
   "end_time": "2025-01-30T09:55:42.408618",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-30T09:41:53.190596",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
