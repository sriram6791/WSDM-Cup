{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf88f3e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-06T18:21:53.757579Z",
     "iopub.status.busy": "2025-01-06T18:21:53.757198Z",
     "iopub.status.idle": "2025-01-06T18:21:54.728383Z",
     "shell.execute_reply": "2025-01-06T18:21:54.727145Z"
    },
    "papermill": {
     "duration": 0.977616,
     "end_time": "2025-01-06T18:21:54.730425",
     "exception": false,
     "start_time": "2025-01-06T18:21:53.752809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mini-wsdm-siamese-network/tensorflow2/mini-version/1/submission.csv\n",
      "/kaggle/input/mini-wsdm-siamese-network/tensorflow2/mini-version/1/best_siamese_model.keras\n",
      "/kaggle/input/wsdm-cup-multilingual-chatbot-arena/sample_submission.csv\n",
      "/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet\n",
      "/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet\n",
      "/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/config.json\n",
      "/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/tokenizer.json\n",
      "/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/metadata.json\n",
      "/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/model.weights.h5\n",
      "/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/assets/tokenizer/vocabulary.spm\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99266fc",
   "metadata": {
    "papermill": {
     "duration": 0.002368,
     "end_time": "2025-01-06T18:21:54.735841",
     "exception": false,
     "start_time": "2025-01-06T18:21:54.733473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# WSDM Cup - Multilingual Chatbot Arena: Human Preference Prediction\n",
    "\n",
    "### Approach for Training a Siamese Neural Network on Multilingual Text Data\n",
    "\n",
    "In this Notebook, we train a **Siamese neural network** to classify which response, **Response A** or **Response B**, is better based on a given **prompt**. The goal is to predict whether **model_a** or **model_b** is the better response based on the context provided by the **prompt**. Here's a step-by-step explanation of the approach used in the code:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Data Preparation**\n",
    "- **Dataset**: The dataset provided contains multiple columns, including `prompt`, `response_a`, `response_b`, and `winner`. \n",
    "  - `prompt`: A question or context for which we have two potential responses (Response A and Response B).\n",
    "  - `response_a` and `response_b`: The responses from two different models to the given prompt.\n",
    "  - `winner`: A label that tells us which response was better, marked as `model_a` or `model_b`.\n",
    "  \n",
    "- **Data Preprocessing**:\n",
    "  - **Concatenation of Prompts and Responses**: For both `response_a` and `response_b`, we concatenate the prompt with each response to prepare the inputs for the Siamese network.\n",
    "  - **Tokenization**: We use the `xlm-roberta-base` tokenizer, a multilingual model, to tokenize and preprocess the text data. This tokenizer handles multiple languages and converts text into tokens that the model can understand.\n",
    "  - **Max Length**: The maximum length for the input sequence is set dynamically based on the training data to ensure that the model can handle long sequences effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Model Architecture**\n",
    "We use a **Siamese network** architecture, which is typically used to compare two inputs and learn their similarity. The network consists of the following components:\n",
    "\n",
    "- **Shared Embedding Layer**: The model uses a shared embedding layer to map both inputs (`response_a` and `response_b`) into the same feature space. This ensures that both inputs are treated equivalently.\n",
    "  \n",
    "- **Bidirectional LSTMs**: We use three layers of **Bidirectional LSTMs (Long Short-Term Memory)** to capture dependencies in the sequence. The bidirectional LSTM processes the input text from both directions (forward and backward) and generates richer representations of the inputs.\n",
    "  \n",
    "- **Dense Layers**: After processing the inputs through the LSTMs, the outputs are concatenated and passed through a series of dense layers with ReLU activations to capture complex patterns.\n",
    "  \n",
    "- **Final Logistic Layer**: The final layer is a **sigmoid activation function** that outputs a probability score indicating whether `response_a` or `response_b` is better, based on the learned features.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Model Training Strategy**\n",
    "- **Multi-GPU Training**: We utilize **TensorFlow's `MirroredStrategy`** to train the model on multiple GPUs, which helps speed up the training process by distributing the workload across available GPUs. This is particularly useful for training large models or handling large datasets.\n",
    "  \n",
    "- **Optimization**: The model is optimized using the **Adam optimizer** with binary cross-entropy loss, as the task is a binary classification problem (predicting whether `response_a` or `response_b` is better).\n",
    "\n",
    "- **Epochs**: The model is trained for a specified number of epochs. Each epoch consists of processing the data in batches and updating the model's weights to minimize the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Callbacks**\n",
    "To improve training efficiency and ensure that the best model is saved, we use the following callbacks:\n",
    "\n",
    "- **ModelCheckpoint**: This callback monitors the validation loss during training and saves the model whenever it improves (i.e., when the validation loss decreases). This ensures that the best version of the model is stored, even if later epochs result in worse performance.\n",
    "  \n",
    "- **EarlyStopping (optional)**: Early stopping can be added to stop training when the model's performance on the validation set stops improving. This prevents overfitting and saves computational resources. In this implementation, early stopping is optional, as we have focused on using the `ModelCheckpoint` to save the best model.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Prediction**\n",
    "Once the model is trained, we use it to predict the better response for the **test dataset**. We process the test data in the same way as the training data (tokenizing the inputs and padding them) and use the trained model to classify whether `response_a` or `response_b` is better. The predictions are then saved in the required format for submission.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of the Workflow:\n",
    "1. **Data Loading & Preprocessing**: Tokenize and prepare the text data (concatenate prompt and responses, handle multi-language input).\n",
    "2. **Model Building**: Construct a Siamese neural network using LSTMs to compare the two responses.\n",
    "3. **Training Strategy**: Use multiple GPUs to train the model efficiently and save the best model using `ModelCheckpoint`.\n",
    "4. **Prediction**: Use the trained model to predict the better response for the test set.\n",
    "\n",
    "This approach ensures that we are leveraging the full potential of the dataset while also optimizing the training process through effective model architecture and callbacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab58118e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T18:21:54.742689Z",
     "iopub.status.busy": "2025-01-06T18:21:54.742087Z",
     "iopub.status.idle": "2025-01-06T18:21:54.748100Z",
     "shell.execute_reply": "2025-01-06T18:21:54.747030Z"
    },
    "papermill": {
     "duration": 0.011475,
     "end_time": "2025-01-06T18:21:54.749908",
     "exception": false,
     "start_time": "2025-01-06T18:21:54.738433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sentencepiece import SentencePieceProcessor\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense, Input, Bidirectional, LSTM, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Enable multi-GPU training\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# # Load and preprocess dataset\n",
    "# data = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')  # Adjust path\n",
    "\n",
    "# # Assuming the dataset has columns: 'prompt', 'response_a', 'response_b', and 'winner'\n",
    "# data['winner'] = data['winner'].map({'model_a': 0, 'model_b': 1})\n",
    "\n",
    "# # Concatenate prompt and responses for tokenization\n",
    "# data['input_a'] = data['prompt'] + \" \" + data['response_a']\n",
    "# data['input_b'] = data['prompt'] + \" \" + data['response_b']\n",
    "\n",
    "# # Load local SentencePiece tokenizer\n",
    "# tokenizer_path = \"/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/assets/tokenizer/vocabulary.spm\"\n",
    "# sp = SentencePieceProcessor()\n",
    "# sp.load(tokenizer_path)\n",
    "\n",
    "# # Define maximum sequence length\n",
    "# max_length = 256  # Adjust as needed\n",
    "\n",
    "# # Tokenize and pad sequences using SentencePiece\n",
    "# def tokenize_and_pad_sentencepiece(texts, max_length):\n",
    "#     # Tokenize and truncate sequences\n",
    "#     tokenized_texts = [sp.encode(text, out_type=int)[:max_length] for text in texts]\n",
    "#     # Pad sequences to ensure consistent length\n",
    "#     padded_texts = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#         tokenized_texts, maxlen=max_length, padding='post', truncating='post'\n",
    "#     )\n",
    "#     return np.array(padded_texts)\n",
    "\n",
    "# # Tokenize input data\n",
    "# tokenized_a = tokenize_and_pad_sentencepiece(data['input_a'].tolist(), max_length)\n",
    "# tokenized_b = tokenize_and_pad_sentencepiece(data['input_b'].tolist(), max_length)\n",
    "\n",
    "# # Prepare labels\n",
    "# labels = data['winner'].values\n",
    "\n",
    "# # Train-test split\n",
    "# X_a_train, X_a_test, X_b_train, X_b_test, y_train, y_test = train_test_split(\n",
    "#     tokenized_a, tokenized_b, labels, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # Define the Siamese network\n",
    "# embedding_dim = 768  # Adjust based on the embedding size of the model\n",
    "\n",
    "# def create_siamese_network():\n",
    "#     # Input layers\n",
    "#     input_a = Input(shape=(max_length,))\n",
    "#     input_b = Input(shape=(max_length,))\n",
    "    \n",
    "#     # Shared embedding layer\n",
    "#     embedding = tf.keras.layers.Embedding(input_dim=sp.vocab_size(), output_dim=embedding_dim, input_length=max_length)\n",
    "    \n",
    "#     # Shared LSTM layers (deeper network)\n",
    "#     shared_lstm1 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "#     shared_lstm2 = Bidirectional(LSTM(32, return_sequences=False))\n",
    "    \n",
    "#     # Process inputs through shared layers\n",
    "#     x_a = embedding(input_a)\n",
    "#     x_a = shared_lstm1(x_a)\n",
    "#     x_a = shared_lstm2(x_a)\n",
    "    \n",
    "#     x_b = embedding(input_b)\n",
    "#     x_b = shared_lstm1(x_b)\n",
    "#     x_b = shared_lstm2(x_b)\n",
    "    \n",
    "#     # Combine outputs\n",
    "#     combined = tf.keras.layers.concatenate([x_a, x_b])\n",
    "#     combined = Dense(32, activation='relu')(combined)\n",
    "#     combined = Dropout(0.5)(combined)\n",
    "#     combined = Dense(16, activation='relu')(combined)\n",
    "#     combined = Dropout(0.5)(combined)\n",
    "    \n",
    "#     # Logistic output\n",
    "#     output = Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "#     # Define the model\n",
    "#     model = Model(inputs=[input_a, input_b], outputs=output)\n",
    "#     return model\n",
    "\n",
    "# # Compile and train the model within the distribution strategy\n",
    "# with strategy.scope():\n",
    "#     model = create_siamese_network()\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Callbacks for early stopping and model checkpointing\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss', patience=3, restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     'best_siamese_model.keras',\n",
    "#     monitor='val_loss',\n",
    "#     save_best_only=True,\n",
    "#     mode='min',\n",
    "#     save_weights_only=False\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     [X_a_train, X_b_train],\n",
    "#     y_train,\n",
    "#     epochs=10,  # Adjust as needed\n",
    "#     batch_size=64,  # Distributed across GPUs\n",
    "#     validation_data=([X_a_test, X_b_test], y_test),\n",
    "#     callbacks=[checkpoint]\n",
    "# )\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate([X_a_test, X_b_test], y_test)\n",
    "# print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = (model.predict([X_a_test, X_b_test]) > 0.5).astype(int)\n",
    "# print(classification_report(y_test, predictions, target_names=['model_a', 'model_b']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e035388f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T18:21:54.756929Z",
     "iopub.status.busy": "2025-01-06T18:21:54.756588Z",
     "iopub.status.idle": "2025-01-06T18:21:54.760269Z",
     "shell.execute_reply": "2025-01-06T18:21:54.759276Z"
    },
    "papermill": {
     "duration": 0.009023,
     "end_time": "2025-01-06T18:21:54.761903",
     "exception": false,
     "start_time": "2025-01-06T18:21:54.752880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training accuracy\n",
    "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2b10fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T18:21:54.769031Z",
     "iopub.status.busy": "2025-01-06T18:21:54.768668Z",
     "iopub.status.idle": "2025-01-06T18:25:36.741622Z",
     "shell.execute_reply": "2025-01-06T18:25:36.740592Z"
    },
    "papermill": {
     "duration": 221.978408,
     "end_time": "2025-01-06T18:25:36.743223",
     "exception": false,
     "start_time": "2025-01-06T18:21:54.764815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Submission file created: 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')  # Adjust path\n",
    "\n",
    "model = load_model('/kaggle/input/mini-wsdm-siamese-network/tensorflow2/mini-version/1/best_siamese_model.keras')\n",
    "# Concatenate prompt and responses for tokenization\n",
    "test_data['input_a'] = test_data['prompt'] + \" \" + test_data['response_a']\n",
    "test_data['input_b'] = test_data['prompt'] + \" \" + test_data['response_b']\n",
    "\n",
    "# Load local SentencePiece tokenizer\n",
    "tokenizer_path = \"/kaggle/input/xlm_roberta/keras/xlm_roberta_base_multi/3/assets/tokenizer/vocabulary.spm\"\n",
    "sp = SentencePieceProcessor()\n",
    "sp.load(tokenizer_path)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = 256  # Ensure it matches the length used during training\n",
    "\n",
    "# Tokenize and pad sequences using SentencePiece\n",
    "def tokenize_and_pad_sentencepiece(texts, max_length):\n",
    "    # Tokenize and truncate sequences\n",
    "    tokenized_texts = [sp.encode(text, out_type=int)[:max_length] for text in texts]\n",
    "    # Pad sequences to ensure consistent length\n",
    "    padded_texts = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_texts, maxlen=max_length, padding='post', truncating='post'\n",
    "    )\n",
    "    return np.array(padded_texts)\n",
    "\n",
    "# Tokenize test data\n",
    "tokenized_test_a = tokenize_and_pad_sentencepiece(test_data['input_a'].tolist(), max_length)\n",
    "tokenized_test_b = tokenize_and_pad_sentencepiece(test_data['input_b'].tolist(), max_length)\n",
    "\n",
    "# Predict winners\n",
    "predictions = model.predict([tokenized_test_a, tokenized_test_b], batch_size=32)\n",
    "predicted_labels = (predictions > 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
    "\n",
    "# Map predictions back to model names\n",
    "test_data['winner'] = np.where(predicted_labels == 0, 'model_a', 'model_b')\n",
    "\n",
    "# Prepare submission file\n",
    "submission = test_data[['id', 'winner']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782598c",
   "metadata": {
    "papermill": {
     "duration": 0.002258,
     "end_time": "2025-01-06T18:25:36.748041",
     "exception": false,
     "start_time": "2025-01-06T18:25:36.745783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "modelId": 2834,
     "modelInstanceId": 4721,
     "sourceId": 206209,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 210479,
     "modelInstanceId": 188461,
     "sourceId": 220977,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 227.771029,
   "end_time": "2025-01-06T18:25:38.573000",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-06T18:21:50.801971",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
